# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'
# %% [markdown]
# # Gathered Notebook
# Gathered from ```c:\Users\Lenovo\Desktop\Git Repo\IBM - Behind The Code\desafio-2-2020\Assets\Notebooks\parte-1.ipynb```
#
# |   |   |
# |---|---|
# |&nbsp;&nbsp;&nbsp|This notebook was generated by the Gather Extension. It requires version 2020.7.94776 (or newer) of the Python Extension, please update [here](https://command:python.datascience.latestExtension). The intent is that it contains only the code and cells required to produce the same results as the cell originally selected for gathering. Please note that the Python analysis is quite conservative, so if it is unsure whether a line of code is necessary for execution, it will err on the side of including it.|
#
# **Are you satisfied with the code that was gathered?**
#
# [Yes](https://command:python.datascience.gatherquality?yes) [No](https://command:python.datascience.gatherquality?no)

# %%
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import minmax_scale
from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import QuantileTransformer
from sklearn.preprocessing import PowerTransformer
from sklearn.tree import ExtraTreeClassifier
from sklearn.svm.classes import OneClassSVM
from sklearn.neural_network.multilayer_perceptron import MLPClassifier
from sklearn.neighbors.classification import RadiusNeighborsClassifier
from sklearn.neighbors.classification import KNeighborsClassifier
from sklearn.multioutput import ClassifierChain
from sklearn.multioutput import MultiOutputClassifier
from sklearn.multiclass import OutputCodeClassifier
from sklearn.multiclass import OneVsOneClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model.stochastic_gradient import SGDClassifier
from sklearn.linear_model.ridge import RidgeClassifierCV
from sklearn.linear_model.ridge import RidgeClassifier
from sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier
from sklearn.gaussian_process.gpc import GaussianProcessClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.ensemble.weight_boosting import AdaBoostClassifier
from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier
from sklearn.ensemble.bagging import BaggingClassifier
from sklearn.ensemble.forest import ExtraTreesClassifier
from sklearn.ensemble.forest import RandomForestClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.calibration import CalibratedClassifierCV
from sklearn.naive_bayes import GaussianNB
from sklearn.semi_supervised import LabelPropagation
from sklearn.semi_supervised import LabelSpreading
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import NearestCentroid
from sklearn.svm import NuSVC
from sklearn.linear_model import Perceptron
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.svm import SVC
# from sklearn.mixture import DPGMM
# from sklearn.mixture import GMM
# from sklearn.mixture import GaussianMixture
# from sklearn.mixture import VBGMM
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier

from numpy.core.arrayprint import printoptions
from sklearn.base import BaseEstimator, TransformerMixin
class DropColumns(BaseEstimator, TransformerMixin):
    def __init__(self, columns):
        self.columns = columns

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        # Primeiro realizamos a cópia do dataframe 'X' de entrada
        data = X.copy()
        # Retornamos um novo dataframe sem as colunas indesejadas
        return data.drop(labels=self.columns, axis='columns')


# %%
from sklearn.model_selection import train_test_split


# %%
import pandas as pd
import os
from pathlib import Path


path = 'C:/Users/Lenovo/Desktop/Git Repo/IBM - Behind The Code/desafio-2-2020/Assets/Data/dataset_desafio_2.csv'
# df_data_1 = pd.read_csv("..\Data\dataset_desafio_2.csv")
df_data_1 = pd.read_csv(path)

from personal_function.Model_Document import cls_Model_Document

obj_document = ''
obj_document = cls_Model_Document()
obj_document.doc_create()

print(obj_document.dict_doc)

# %%
import matplotlib.pyplot as plt
import seaborn as sns


# %%
fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))
sns.countplot(ax=axes[0], x='REPROVACOES_DE', data=df_data_1)
sns.countplot(ax=axes[1], x='REPROVACOES_EM', data=df_data_1)
sns.countplot(ax=axes[2], x='REPROVACOES_MF', data=df_data_1)
sns.countplot(ax=axes[3], x='REPROVACOES_GO', data=df_data_1)


# %%
fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))
sns.distplot(df_data_1['NOTA_DE'], ax=axes[0])
sns.distplot(df_data_1['NOTA_EM'], ax=axes[1])
sns.distplot(df_data_1['NOTA_MF'], ax=axes[2])


# %%
fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))
sns.countplot(ax=axes[0], x='INGLES', data=df_data_1)
sns.countplot(ax=axes[1], x='FALTAS', data=df_data_1)
sns.countplot(ax=axes[2], x='H_AULA_PRES', data=df_data_1)
sns.countplot(ax=axes[3], x='TAREFAS_ONLINE', data=df_data_1)


# %%
sns.countplot(x='PERFIL', data=df_data_1)


# %%
from sklearn.base import BaseEstimator, TransformerMixin
class DropColumns(BaseEstimator, TransformerMixin):
    def __init__(self, columns):
        self.columns = columns

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        # Primeiro realizamos a cópia do dataframe 'X' de entrada
        data = X.copy()
        # Retornamos um novo dataframe sem as colunas indesejadas
        return data.drop(labels=self.columns, axis='columns')


# %%
list_column_remove = ["NOME"]
obj_document.doc_add_model(list_column_remove)
# list_column_remove = ["NOME",'MATRICULA']
rm_columns = DropColumns(
    columns=list_column_remove  # Essa transformação recebe como parâmetro uma lista com os nomes das colunas indesejadas
)


# %%
rm_columns.fit(X=df_data_1)
df_data_2 = pd.DataFrame.from_records(
    data=rm_columns.transform(
        X=df_data_1
    ),
)


# %%
from personal_function.Model_Converter_CatNum import convert_cat_to_num
df_key = 'PERFIL'
convert_cat_to_num(df_data_2,df_key)


# %%
from personal_function.Model_Missing_Value_Imputer import Missing_Value_imputer
si = Missing_Value_imputer()


# %%
si.fit(X=df_data_2)
df_data_3 = pd.DataFrame.from_records(
    data=si.transform(
        X=df_data_2
    ),  # o resultado SimpleImputer.transform(<<pandas dataframe>>) é lista de listas
    columns=df_data_2.columns  # as colunas originais devem ser conservadas nessa transformação
)


# %%
target = ["PERFIL"]
list_column_remove.append(target[0])
features = [i for i in df_data_3.columns if i not in list_column_remove ]
X = df_data_3[features]
y = df_data_3[target]


# %%
from personal_function.Model_Converter_CatNum import convert_num_to_cat
df_key = 'PERFIL'
cat_bin = [1,2,3,4,5]
cat_label = [
            'DIFICULDADE'
            ,'EXATAS'
            ,'EXCELENTE'
            ,'HUMANAS'
            ,'MUITO_BOM'
        ]
convert_num_to_cat(y,df_key,cat_bin,cat_label)


# %%
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)


# %%
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# scaler = [StandardScaler()]
classifier_test = [SVC()]


#=================Scaler
scaler = [
StandardScaler(),
MinMaxScaler(),
MaxAbsScaler(),
RobustScaler(quantile_range=(25, 75)),
PowerTransformer(method='yeo-johnson'),
# PowerTransformer(method='box-cox'),
QuantileTransformer(output_distribution='normal'),
QuantileTransformer(output_distribution='uniform'),
Normalizer()
]

# %%

#=================Classifier
classifier_test = [
OneVsRestClassifier(SVC())
,DecisionTreeClassifier(max_depth=5)
,SVC()
,SVC(kernel="linear", C=0.025)
,LogisticRegressionCV(cv=5, random_state=0)
,GradientBoostingClassifier(random_state=0)
,BaggingClassifier(base_estimator=SVC(),n_estimators=10, random_state=0).fit(X, y)
,ExtraTreesClassifier(n_estimators=100, random_state=0)
,HistGradientBoostingClassifier()
,MLPClassifier(random_state=1, max_iter=300)
,OneVsOneClassifier(LinearSVC(random_state=0))
,OutputCodeClassifier(estimator=RandomForestClassifier(random_state=0),random_state=0)
]
print('Importacao OK')
#%%
count = 0
dict_test = {}
dict_all = {}
for i in range(len(scaler)):
        scaler_i = scaler[i]
        for j in range(len(classifier_test)):
            count += 1
            classifier_test_j = classifier_test[j]
            dict_all[f'{count}'] = f'S{i} | C{j}'
print('==============================')
print(f'Scaler: {len(scaler)}')
print(f'Classifier: {len(classifier_test)}')
# print(dict_all)
# print('==============================')
# dict_all.pop('1')
# print(dict_all)

# %%
dict_final_doc = {}
count_model = 0
print(dict_all.keys())
# try:
for i in range(len(scaler)):
    scaler_i = scaler[i]
    for j in range(len(classifier_test)):
        count_model += 1
        classifier_test_j = classifier_test[j]
        classifier_model = Pipeline([('scaler', scaler_i), ('svc', classifier_test_j)])

        new_id = f'scaler: {i} | classifier: {j} | Model:{count_model}'
        obj_document.doc_set_id(new_id)
        dict_final_doc[f'{count}'] = obj_document
        ## %%
        classifier_model.fit(
            X_train,
            y_train
        )


        ## %%
        y_pred = classifier_model.predict(X_test)


        ## %%
        from sklearn.metrics import accuracy_score
        print(count_model)
        print("Acurácia: {}%".format(100*round(accuracy_score(y_test, y_pred), 2)))


        ## %%
        accuracy = 100*round(accuracy_score(y_test,y_pred),2)
        file_name = 'Accuracy'



        dict_final_doc[f'{count}'].doc_set_id(f'{accuracy}')
        dict_final_doc[f'{count}'].doc_add_accuracy(accuracy)
        dict_final_doc[f'{count}'].doc_add_model(accuracy)

        path = os.getcwd()
        key = dict_final_doc[f'{count}'].dict_doc.get('id')
        acc = classifier_model
        print()
        print(f'''
        id: {dict_final_doc[f'{count}'].dict_doc['param']['id']}
        '''
        )


        if accuracy < 80:
            dict_all.pop(str(count_model), None)
        else:
            dict_test[f'M{count_model} | {accuracy}'] = new_id

#%%

import json
file_name = 'test_result'
with open(f"{file_name}.json","w") as file:
    json.dump(dict_test, file)

# except Exception as e:
#     new_id = f'scaler: {i} - classifier: {j}'
#     print(f' - scaler: {len(scaler)} \n - classifier: {len(classifier_test)}')
#     print(f'falha aqui: {new_id}')
#     print(e)
# %%
